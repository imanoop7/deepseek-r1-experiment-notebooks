{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and processing PDFs...\n",
      "Created 53 chunks from PDFs\n",
      "Creating vector store...\n",
      "Clearing existing vector store at chroma_db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\deepseek-r1-experiment-notebooks\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vector store...\n",
      "Vector store created and persisted at chroma_db\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def load_and_process_pdfs(data_dir: str):\n",
    "    \"\"\"Load PDFs from directory and split into chunks.\"\"\"\n",
    "    loader = DirectoryLoader(\n",
    "        data_dir,\n",
    "        glob=\"**/*.pdf\",\n",
    "        loader_cls=PyPDFLoader\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    return chunks\n",
    "\n",
    "def create_vector_store(chunks, persist_directory: str):\n",
    "    \"\"\"Create and persist Chroma vector store.\"\"\"\n",
    "    # Clear existing vector store if it exists\n",
    "    if os.path.exists(persist_directory):\n",
    "        print(f\"Clearing existing vector store at {persist_directory}\")\n",
    "        shutil.rmtree(persist_directory)\n",
    "    \n",
    "    # Initialize HuggingFace embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "        model_kwargs={'device': 'cpu'}\n",
    "    )\n",
    "    \n",
    "    # Create and persist Chroma vector store\n",
    "    print(\"Creating new vector store...\")\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    return vectordb\n",
    "\n",
    "def main():\n",
    "    # Define directories\n",
    "    data_dir =\"data\"\n",
    "    db_dir = \"chroma_db\"\n",
    "    \n",
    "    # Process PDFs\n",
    "    print(\"Loading and processing PDFs...\")\n",
    "    chunks = load_and_process_pdfs(data_dir)\n",
    "    print(f\"Created {len(chunks)} chunks from PDFs\")\n",
    "    \n",
    "    # Create vector store\n",
    "    print(\"Creating vector store...\")\n",
    "    vectordb = create_vector_store(chunks, db_dir)\n",
    "    print(f\"Vector store created and persisted at {db_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://7a9c0241555213ddd8.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://7a9c0241555213ddd8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">hi</span>                                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ OpenAIServerModel - deepseek-r1:1.5b ──────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mhi\u001b[0m                                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m OpenAIServerModel - deepseek-r1:1.5b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Error in generating tool call with model:</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Error code: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">400</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> - {</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">'error'</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">: {</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">'message'</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">'registry.ollama.ai/library/deepseek-r1:1.5b does not support tools'</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">, </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">'type'</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">'api_error'</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">, </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">'param'</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">None</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">, </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">'code'</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">None</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">}}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mError in generating tool call with model:\u001b[0m\n",
       "\u001b[1;31mError code: \u001b[0m\u001b[1;31m400\u001b[0m\u001b[1;31m - \u001b[0m\u001b[1;31m{\u001b[0m\u001b[1;31m'error'\u001b[0m\u001b[1;31m: \u001b[0m\u001b[1;31m{\u001b[0m\u001b[1;31m'message'\u001b[0m\u001b[1;31m: \u001b[0m\u001b[1;31m'registry.ollama.ai/library/deepseek-r1:1.5b does not support tools'\u001b[0m\u001b[1;31m, \u001b[0m\n",
       "\u001b[1;31m'type'\u001b[0m\u001b[1;31m: \u001b[0m\u001b[1;31m'api_error'\u001b[0m\u001b[1;31m, \u001b[0m\u001b[1;31m'param'\u001b[0m\u001b[1;31m: \u001b[0m\u001b[1;3;31mNone\u001b[0m\u001b[1;31m, \u001b[0m\u001b[1;31m'code'\u001b[0m\u001b[1;31m: \u001b[0m\u001b[1;3;31mNone\u001b[0m\u001b[1;31m}\u001b[0m\u001b[1;31m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 0: Duration 2.13 seconds]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 0: Duration 2.13 seconds]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\GitHub\\deepseek-r1-experiment-notebooks\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 715, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub\\deepseek-r1-experiment-notebooks\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub\\deepseek-r1-experiment-notebooks\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2088, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub\\deepseek-r1-experiment-notebooks\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1647, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub\\deepseek-r1-experiment-notebooks\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 728, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub\\deepseek-r1-experiment-notebooks\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 722, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub\\deepseek-r1-experiment-notebooks\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub\\deepseek-r1-experiment-notebooks\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2461, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub\\deepseek-r1-experiment-notebooks\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 962, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub\\deepseek-r1-experiment-notebooks\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 705, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub\\deepseek-r1-experiment-notebooks\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 866, in gen_wrapper\n",
      "    response = next(iterator)\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub\\deepseek-r1-experiment-notebooks\\.venv\\Lib\\site-packages\\smolagents\\gradio_ui.py\", line 197, in interact_with_agent\n",
      "    for msg in stream_to_gradio(self.agent, task=prompt, reset_agent_memory=False):\n",
      "  File \"d:\\GitHub\\deepseek-r1-experiment-notebooks\\.venv\\Lib\\site-packages\\smolagents\\gradio_ui.py\", line 145, in stream_to_gradio\n",
      "    total_input_tokens += agent.model.last_input_token_count\n",
      "TypeError: unsupported operand type(s) for +=: 'int' and 'NoneType'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://7a9c0241555213ddd8.gradio.live\n"
     ]
    }
   ],
   "source": [
    "from smolagents import OpenAIServerModel, CodeAgent, ToolCallingAgent, HfApiModel, tool, GradioUI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "reasoning_model_id = \"deepseek-r1:7b\"\n",
    "\n",
    "def get_model(model_id):\n",
    "        return OpenAIServerModel(\n",
    "            model_id=model_id,\n",
    "            api_base=\"http://localhost:11434/v1\",\n",
    "            api_key=\"ollama\"\n",
    "        )\n",
    "\n",
    "# Create the reasoner for better RAG\n",
    "reasoning_model = get_model(reasoning_model_id)\n",
    "reasoner = CodeAgent(tools=[], model=reasoning_model, add_base_tools=False, max_steps=2)\n",
    "\n",
    "# Initialize vector store and embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "db_dir = \"chroma_db\"\n",
    "vectordb = Chroma(persist_directory=db_dir, embedding_function=embeddings)\n",
    "\n",
    "@tool\n",
    "def rag_with_reasoner(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    This is a RAG tool that takes in a user query and searches for relevant content from the vector database.\n",
    "    The result of the search is given to a reasoning LLM to generate a response, so what you'll get back\n",
    "    from this tool is a short answer to the user's question based on RAG context.\n",
    "\n",
    "    Args:\n",
    "        user_query: The user's question to query the vector database with.\n",
    "    \"\"\"\n",
    "    # Search for relevant documents\n",
    "    docs = vectordb.similarity_search(user_query, k=3)\n",
    "    \n",
    "    # Combine document contents\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    # Create prompt with context\n",
    "    prompt = f\"\"\"Based on the following context, answer the user's question. Be concise and specific.\n",
    "    If there isn't sufficient information, give as your answer a better query to perform RAG with.\n",
    "    \n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {user_query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Get response from reasoning model\n",
    "    response = reasoner.run(prompt, reset=False)\n",
    "    return response\n",
    "\n",
    "# Create the primary agent to direct the conversation\n",
    "tool_model = get_model(reasoning_model_id)\n",
    "primary_agent = ToolCallingAgent(tools=[rag_with_reasoner], model=tool_model, add_base_tools=False, max_steps=3)\n",
    "\n",
    "# Example prompt: Compare and contrast the services offered by RankBoost and Omni Marketing\n",
    "def main():\n",
    "    GradioUI(primary_agent).launch()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
